{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit_hash': 'd0b255042',\n",
      " 'commit_source': 'installation',\n",
      " 'default_encoding': 'cp1252',\n",
      " 'ipython_path': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython',\n",
      " 'ipython_version': '7.13.0',\n",
      " 'os_name': 'nt',\n",
      " 'platform': 'Windows-10-10.0.16299-SP0',\n",
      " 'sys_executable': 'C:\\\\ProgramData\\\\Anaconda3\\\\python.exe',\n",
      " 'sys_platform': 'win32',\n",
      " 'sys_version': '3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit '\n",
      "                '(AMD64)]'}\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import IPython\n",
    "print(IPython.sys_info())\n",
    "\n",
    "\n",
    "#c.Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12532</th>\n",
       "      <td>12534</td>\n",
       "      <td>This was unusual: a modern-day film which was ...</td>\n",
       "      <td>Isso era incomum: um filme moderno que era ult...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35445</th>\n",
       "      <td>35447</td>\n",
       "      <td>Some of my old friends suggested me to watch t...</td>\n",
       "      <td>Alguns dos meus velhos amigos sugeriram que eu...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20279</th>\n",
       "      <td>20281</td>\n",
       "      <td>What a pleasure. This is really a parody. Only...</td>\n",
       "      <td>Que prazer. Isto é realmente uma paródia. Some...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>2971</td>\n",
       "      <td>There are about ten minutes about half way thr...</td>\n",
       "      <td>Há cerca de dez minutos a meio da Strangeland,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45161</th>\n",
       "      <td>45163</td>\n",
       "      <td>Othello, the classic Shakespearen story of lov...</td>\n",
       "      <td>Otelo, a clássica história de Shakespearen sob...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "12532  12534  This was unusual: a modern-day film which was ...   \n",
       "35445  35447  Some of my old friends suggested me to watch t...   \n",
       "20279  20281  What a pleasure. This is really a parody. Only...   \n",
       "2969    2971  There are about ten minutes about half way thr...   \n",
       "45161  45163  Othello, the classic Shakespearen story of lov...   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "12532  Isso era incomum: um filme moderno que era ult...       pos  \n",
       "35445  Alguns dos meus velhos amigos sugeriram que eu...       neg  \n",
       "20279  Que prazer. Isto é realmente uma paródia. Some...       pos  \n",
       "2969   Há cerca de dez minutos a meio da Strangeland,...       neg  \n",
       "45161  Otelo, a clássica história de Shakespearen sob...       pos  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://dados-ml-pln.s3-sa-east-1.amazonaws.com/imdb-reviews-pt-br.csv'\n",
    "  ).sample(10000, random_state=42)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of           id                                            text_en  \\\n",
       "12532  12534  This was unusual: a modern-day film which was ...   \n",
       "35445  35447  Some of my old friends suggested me to watch t...   \n",
       "20279  20281  What a pleasure. This is really a parody. Only...   \n",
       "2969    2971  There are about ten minutes about half way thr...   \n",
       "45161  45163  Othello, the classic Shakespearen story of lov...   \n",
       "...      ...                                                ...   \n",
       "25219  25221  Did I step in something or is that bad smell c...   \n",
       "30693  30695  A stuttering plot, uninteresting characters an...   \n",
       "20915  20917  In a sport that prizes quirkiness and treasure...   \n",
       "14933  14935  John Huston finished his remarkable career wit...   \n",
       "45388  45390  gone in 60 sec. where do i began, it keeps you...   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "12532  Isso era incomum: um filme moderno que era ult...       pos  \n",
       "35445  Alguns dos meus velhos amigos sugeriram que eu...       neg  \n",
       "20279  Que prazer. Isto é realmente uma paródia. Some...       pos  \n",
       "2969   Há cerca de dez minutos a meio da Strangeland,...       neg  \n",
       "45161  Otelo, a clássica história de Shakespearen sob...       pos  \n",
       "...                                                  ...       ...  \n",
       "25219  Eu pisei em algo ou esse cheiro ruim vem do Da...       neg  \n",
       "30693  Um enredo gaguejante, personagens desinteressa...       neg  \n",
       "20915  Em um esporte que preza a estranheza e valoriz...       pos  \n",
       "14933  John Huston terminou sua notável carreira com ...       pos  \n",
       "45388  ido em 60 seg. onde eu comecei, ele mantém voc...       pos  \n",
       "\n",
       "[10000 rows x 4 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "text_en      0\n",
       "text_pt      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df.dropna(inplace=True)\n",
    "#df[\"texto\"] = df['nome'] + \" \" + df['descricao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    5088\n",
       "pos    4912\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "RMG-cnHho8pi",
    "outputId": "4915f92b-b9f4-45ab-e3e6-4125e8a8a66b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BRJUVEN1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 65881)\n",
      "0.705\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 1: Vetorização por contagem de termos simples com unigrama, sem stopwords do NLTK e modelo de classificação DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "# stopwords NLTK\n",
    "nltk.download('stopwords')\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# vetorização por contagem de termos\n",
    "#vect = CountVectorizer(ngram_range=(1,1)) # exemplo 1.1: vetorização unigrama com stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # exemplo 1.2: vetorização unigrama sem stopwords\n",
    "vect.fit(df.text_pt)\n",
    "text_vect = vect.transform(df.text_pt)\n",
    "\n",
    "# divisão da amostra entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      text_vect, \n",
    "      df[\"sentiment\"], \n",
    "      test_size = 0.3, \n",
    "      random_state = 42\n",
    "  )\n",
    "\n",
    "# treinamento do modelo ávore de decisão\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# escoragem da classificação na amostra de teste\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiFf6cRWXRkC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (46.4.0.post20200518)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.46.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "Requirement already satisfied: pt_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz#egg=pt_core_news_sm==2.2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (46.4.0.post20200518)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.46.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.1.0)\n",
      "symbolic link created for C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\data\\pt <<===>> C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pt_core_news_sm\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n",
      "[+] Linking successful\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pt_core_news_sm -->\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\data\\pt\n",
      "You can now load the model via spacy.load('pt')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pt\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBkhv7TJpPRU"
   },
   "outputs": [],
   "source": [
    "# função de lematização completa do documento\n",
    "def lemmatizer_text(text):\n",
    "  sent = []\n",
    "  doc = nlp(text)\n",
    "  for word in doc:\n",
    "      sent.append(word.lemma_)\n",
    "  return \" \".join(sent)\n",
    "\n",
    "# função de lematização para os verbos do documento\n",
    "def lemmatizer_verbs(text):\n",
    "  sent = []\n",
    "  doc = nlp(text)\n",
    "  for word in doc:\n",
    "      if word.pos_ == \"VERB\":\n",
    "          sent.append(word.lemma_)\n",
    "      else:\n",
    "          sent.append(word.text)\n",
    "  return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YV5GVlPFq-rG",
    "outputId": "efdc7bb0-fa45-4a28-dc75-7e907aac9c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correr 1 , 2 , 3\n",
      "correr 1 , 2 , 3\n"
     ]
    }
   ],
   "source": [
    "# teste das funções de lematização\n",
    "#!pip install spacy\n",
    "#!python -m spacy download pt\n",
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "# validação das funções\n",
    "print(lemmatizer_text('correndo 1, 2, 3'))\n",
    "print(lemmatizer_verbs('correndo 1, 2, 3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLz7qvbj2WVm"
   },
   "outputs": [],
   "source": [
    "# aplica a lematização no dataframe criando novas colunas\n",
    "df['text_lemma'] = df.text_pt.apply(lemmatizer_text)\n",
    "df['text_lemma_verbs'] = df.text_pt.apply(lemmatizer_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "n8jQNEBhUeOW",
    "outputId": "44ecdd1f-7288-4480-a0d2-75748aaa80d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 12532 to 45388\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                10000 non-null  int64 \n",
      " 1   text_en           10000 non-null  object\n",
      " 2   text_pt           10000 non-null  object\n",
      " 3   sentiment         10000 non-null  object\n",
      " 4   text_lemma        10000 non-null  object\n",
      " 5   text_lemma_verbs  10000 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 546.9+ KB\n",
      "None\n",
      "\n",
      "shape:  (10000, 6)\n",
      "          id                                            text_en  \\\n",
      "12532  12534  This was unusual: a modern-day film which was ...   \n",
      "35445  35447  Some of my old friends suggested me to watch t...   \n",
      "\n",
      "                                                 text_pt sentiment  \\\n",
      "12532  Isso era incomum: um filme moderno que era ult...       pos   \n",
      "35445  Alguns dos meus velhos amigos sugeriram que eu...       neg   \n",
      "\n",
      "                                              text_lemma  \\\n",
      "12532  Isso ser incomum : um filmar moderno que ser u...   \n",
      "35445  Alguns dos meu velho amigo sugerir que eu assi...   \n",
      "\n",
      "                                        text_lemma_verbs  \n",
      "12532  Isso ser incomum : um filme moderno que ser ul...  \n",
      "35445  Alguns dos meus velhos amigos sugerir que eu a...  \n"
     ]
    }
   ],
   "source": [
    "# análise dos dados e nova estrutura do dataframe\n",
    "print(df.info())\n",
    "print('\\nshape: ', df.shape)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3rabyoXjzQCn",
    "outputId": "8789c8e6-c6f6-48ad-d72f-78fb431cb675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este é um exemplo do motivo pelo qual a maioria dos filmes de ação são os mesmos. Genérico e chato, não há nada que valha a pena assistir aqui. Um completo desperdício dos talentos de Ice-T e Cubo de Gelo que foram mal aproveitados, cada um comprovando que são capazes de atuar e agir bem. Não se incomode com este, vá ver New Jack City, Ricochet ou assistir New York Undercover para Ice-T, ou Boyz no Hood, Higher Learning ou Friday for Ice Cube e ver o negócio real. Ice-Ts horrivelmente clichê diálogo sozinho faz este filme ralar os dentes, e eu ainda estou me perguntando o que diabos Bill Paxton estava fazendo neste filme? E por que diabos ele sempre interpreta exatamente o mesmo personagem? Dos extraterrestres em diante, todos os filmes que eu vi com Bill Paxton o fizeram interpretar exatamente o mesmo personagem irritante, e pelo menos em Aliens seu personagem morreu, o que o tornou um pouco gratificante ... No geral, esse é lixo de ação de segunda classe. Existem incontáveis ​​filmes melhores para ver, e se você realmente quiser ver esse filme, assista a Judgment Night, que é praticamente uma cópia carbono, mas tem melhor atuação e um roteiro melhor. A única coisa que fez isso valer a pena assistir foi uma mão decente na câmera - a cinematografia era quase refrescante, o que chega perto de compensar o horrível filme em si - mas não é bem assim. 4/10\n",
      "Este ser um exemplo do motivar pelar qual o maioria dos filme de ação ser o mesmo . Genérico e chato , não haver nado que valer o peno assistir aqui . Um completar desperdício dos talento de Ice-T e Cubo de Gelo que ser mal aproveitar , cada um comprovar que ser capaz de atuar e agir bem . Não se incomodar com este , ir ver New Jack City , Ricochet ou assistir New York Undercover parir Ice-T , ou Boyz o Hood , Higher Learning ou Friday ser Ice Cube e ver o negócio real . Ice-Ts horrivelmente clichê diálogo só fazer este filmar ralar o dente , e eu ainda estar me perguntar o que diabo Bill Paxton estar fazer n este filmar ? E por que diabo ele sempre interpretar exatamente o mesmo personagem ? Dos extraterrestre em diante , todo o filme que eu vir com Bill Paxton o fazer interpretar exatamente o mesmo personagem irritante , e pelar menos em Aliens seu personagem morrer , o que o tornar um pouco gratificante ... No geral , esse ser lixar de ação de segundo classe . Existem incontável ​​filmes melhorar parir ver , e se você realmente querer ver esse filmar , assistir o Judgment Night , que ser praticamente umar cópia carbono , mas ter melhor atuação e um roteiro melhor . A único coisa que fazer isso valer o peno assistir ser umar mão decente o câmera - o cinematografia ser quase refrescante , o que chegar perto de compensar o horrível filmar em si - mas não ser bem assim . 4/10\n",
      "Este ser um exemplo do motivo pelo qual a maioria dos filmes de ação ser os mesmos . Genérico e chato , não haver nada que valer a pena assistir aqui . Um completo desperdício dos talentos de Ice-T e Cubo de Gelo que foram mal aproveitar , cada um comprovando que ser capazes de atuar e agir bem . Não se incomodar com este , vá ver New Jack City , Ricochet ou assistir New York Undercover para Ice-T , ou Boyz no Hood , Higher Learning ou Friday for Ice Cube e ver o negócio real . Ice-Ts horrivelmente clichê diálogo sozinho fazer este filme ralar os dentes , e eu ainda estar me perguntar o que diabos Bill Paxton estava fazer n este filme ? E por que diabos ele sempre interpretar exatamente o mesmo personagem ? Dos extraterrestres em diante , todos os filmes que eu vir com Bill Paxton o fazer interpretar exatamente o mesmo personagem irritante , e pelo menos em Aliens seu personagem morrer , o que o tornar um pouco gratificante ... No geral , esse ser lixo de ação de segunda classe . Existem incontáveis ​​filmes melhores para ver , e se você realmente querer ver esse filme , assista a Judgment Night , que ser praticamente uma cópia carbono , mas ter melhor atuação e um roteiro melhor . A única coisa que fazer isso valer a pena assistir ser uma mão decente na câmera - a cinematografia ser quase refrescante , o que chegar perto de compensar o horrível filme em si - mas não ser bem assim . 4/10\n"
     ]
    }
   ],
   "source": [
    "# análise para comparação dos textos\n",
    "print(df['text_pt'][1])\n",
    "print(df['text_lemma'][1])\n",
    "print(df['text_lemma_verbs'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YcUoUN-UsiTf",
    "outputId": "5ce81bea-e0db-4184-bec1-4dee0c85bfe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 689498)\n",
      "0.7185\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 2: vetorização por contagem de termos simples com a combinação de unigrama e bigrama no documento lematizado, sem stopwords do SpaCy e modelo de classificação DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "\n",
    "# stopwords SpaCy\n",
    "nlp = spacy.load('pt')\n",
    "stops = nlp.Defaults.stop_words\n",
    "\n",
    "# vetorização por contagem de termos no documento lematizado\n",
    "#vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # exemplo 2.1: vetorização e combinação de unigrama sem stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # exemplo 2.2: vetorização e combinação de unigrama e bigrama sem stopwords\n",
    "vect.fit(df.text_lemma)\n",
    "text_vect = vect.transform(df.text_lemma)\n",
    "\n",
    "# divisão da amostra entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      text_vect, \n",
    "      df[\"sentiment\"], \n",
    "      test_size = 0.2, \n",
    "      random_state = 42\n",
    "  )\n",
    "\n",
    "# treinamento do modelo ávore de decisão\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# escoragem da classificação na amostra de teste\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vsjOD_P5hAIc",
    "outputId": "15dda9d6-b8e4-4e3d-8411-b8afa6a8b134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 756159)\n",
      "0.7095\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 3: Vetorização por contagem de termos simples com a combinação de unigrama e bigrama no documento com verbos lematizado, sem stopwords do SpaCy e modelo de classificação DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "\n",
    "# stopwords SpaCy\n",
    "nlp = spacy.load('pt')\n",
    "stops = nlp.Defaults.stop_words\n",
    "\n",
    "# vetorização por contagem de termos no documento com os verbos lematizado\n",
    "#vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # exemplo 3.1: vetorização e combinação de unigrama sem stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # exemplo 3.2: vetorização e combinação de unigrama e bigrama sem stopwords\n",
    "vect.fit(df.text_lemma_verbs)\n",
    "text_vect = vect.transform(df.text_lemma_verbs)\n",
    "\n",
    "# divisão da amostra entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      text_vect, \n",
    "      df[\"sentiment\"], \n",
    "      test_size = 0.2, \n",
    "      random_state = 42\n",
    "  )\n",
    "\n",
    "# treinamento do modelo ávore de decisão\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# escoragem da classificação na amostra de teste\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "CG51OO8KlduJ",
    "outputId": "2adb05d7-e7fa-43e9-ff06-88486abc55e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BRJUVEN1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 752260)\n",
      "0.71\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 4: Vetorização por contagem de termos simples com a combinação de unigrama e bigrama no documento com verbos lematizado, sem stopwords do SpaCy e NLTK combinadas e modelo de classificação DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "# stopwords do SpaCy e NLTK combinadas\n",
    "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
    "#len(stops)\n",
    "\n",
    "# vetorização por contagem de termos no documento com os verbos lematizado\n",
    "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # exemplo 4.1: vetorização e combinação de unigrama e bigrama sem stopwords NLTK e Spacy\n",
    "vect.fit(df.text_lemma_verbs)\n",
    "text_vect = vect.transform(df.text_lemma_verbs)\n",
    "\n",
    "# divisão da amostra entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      text_vect, \n",
    "      df[\"sentiment\"], \n",
    "      test_size = 0.2, \n",
    "      random_state = 42\n",
    "  )\n",
    "\n",
    "# treinamento do modelo ávore de decisão\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# escoragem da classificação na amostra de teste\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qGTuFeHdMLCX",
    "outputId": "10154b47-3a0e-4752-aa4e-da8ac7376ad4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BRJUVEN1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 48648)\n",
      "0.693\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 5: Vetorização por contagem de termos TF-IDF com a combinação de unigrama com documentos lematizado, sem stopwords do SpaCy e NLTK combinadas e modelo de classificação DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "# stopwords do SpaCy e NLTK combinadas\n",
    "stops_spacy = nlp.Defaults.stop_words\n",
    "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "stops = list(set(stops_spacy).union(set(stops_nltk)))\n",
    "\n",
    "# vetorização por contagem de termos no documento lematizado\n",
    "vetorTfidf = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops, norm='l2') # exemplo 4.1: vetorização tf-idf e combinação de unigrama sem stopwords NLTK e Spacy\n",
    "#vetorTfidf = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops_spacy, norm='l2') # exemplo 4.2: vetorização tf-idf e combinação de unigrama e bigrama sem stopwords NLTK e Spacy\n",
    "#vetorTfidf = TfidfVectorizer(ngram_range=(1,2), use_idf=False, stop_words=stops_spacy, norm='l1') # exemplo 4.3: vetorização tf e combinação de unigrama e bigrama sem stopwords NLTK e Spacy\n",
    "vetorTfidf.fit(df.text_lemma)\n",
    "text_vect = vetorTfidf.transform(df.text_lemma)\n",
    "\n",
    "# divisão da amostra entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      text_vect, \n",
    "      df[\"sentiment\"], \n",
    "      test_size = 0.2, \n",
    "      random_state = 42\n",
    "  )\n",
    "\n",
    "# treinamento do modelo ávore de decisão\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# escoragem da classificação na amostra de teste\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bC8VbMbL2Lou",
    "outputId": "d4c35cbf-918b-49f9-cbfe-f94d443756c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BRJUVEN1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 56705)\n",
      "0.863\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 6: Vetorização por contagem de termos TF-IDF com a combinação de unigrama com documentos com verbos lematizado, sem stopwords do SpaCy e NLTK combinadas e modelo de classificação Regressão Logistica\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "# stopwords do SpaCy e NLTK combinadas\n",
    "stops_spacy = nlp.Defaults.stop_words\n",
    "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "stops = list(set(stops_spacy).union(set(stops_nltk)))\n",
    "\n",
    "# vetorização por contagem de termos no documento lematizado\n",
    "vetorTfidf = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops, norm='l2')\n",
    "vetorTfidf.fit(df.text_lemma_verbs)\n",
    "text_vect = vetorTfidf.transform(df.text_lemma_verbs)\n",
    "\n",
    "# treinamento do modelo regressão logística\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# escoragem da classificação na amostra de teste\n",
    "y_prediction = model.predict(X_test)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)\n",
    "\n",
    "# Nosso melhor modelo até aqui!!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
